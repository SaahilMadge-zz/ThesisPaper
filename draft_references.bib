@article{Bachman2014,
abstract = {We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout [9] in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We examine the relationship of pseudo-ensembles, which involve perturbation in model-space, to standard ensemble methods and existing notions of robustness, which focus on perturbation in observation-space. We present a novel regular- izer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer nat- urally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of [19] into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.4864v1},
author = {Bachman, Philip and Alsharif, Ouais and Precup, Doina},
eprint = {arXiv:1412.4864v1},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Bachman, Alsharif, Precup - 2014 - Learning with Pseudo-Ensembles.pdf:pdf},
journal = {Business},
pages = {1--9},
title = {{Learning with Pseudo-Ensembles}},
year = {2014}
}
@article{Berant2014,
author = {Berant, Jonathan and Clark, Peter},
file = {:Users/SaahilM/Documents/Princeton/Academics/Thesis/ResearchPapers/berant-srikumar-manning-emnlp14.pdf:pdf},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
pages = {1499--1510},
title = {{Modeling Biological Processes for Reading Comprehension}},
url = {http://allenai.org/content/publications/berant-srikumar-manning-emnlp14.pdf},
year = {2014}
}
@article{Bishop2006,
  title={Pattern Recognition},
  author={Bishop, Christopher M},
  journal={Machine Learning},
  year={2006}
}
@article{Bordes2014,
abstract = {This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields state-of-the-art results on a competitive benchmark of the literature.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.3676v1},
author = {Bordes, Antoine and Chopra, Sumit and Weston, Jason},
eprint = {arXiv:1406.3676v1},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Bordes, Chopra, Weston - 2014 - Question Answering with Subgraph Embeddings.pdf:pdf},
journal = {arXiv preprint arXiv:1406.3676},
number = {1},
pages = {615--620},
title = {{Question Answering with Subgraph Embeddings}},
url = {http://arxiv.org/abs/1406.3676},
year = {2014}
}
@article{Bordes2015,
abstract = {Training large-scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions. This paper studies the impact of multitask and transfer learning for simple question answering; a setting for which the reasoning required to answer is quite easy, as long as one can retrieve the correct evidence given a question, which can be difficult in large-scale conditions. To this end, we introduce a new dataset of 100k questions that we use in conjunction with existing benchmarks. We conduct our study within the framework of Memory Networks (Weston et al., 2015) because this perspective allows us to eventually scale up to more complex reasoning, and show that Memory Networks can be successfully trained to achieve excellent performance.},
archivePrefix = {arXiv},
arxivId = {1506.02075},
author = {Bordes, Antoine and Usunier, Nicolas and Chopra, Sumit and Weston, Jason},
eprint = {1506.02075},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Bordes et al. - 2015 - Large-scale Simple Question Answering with Memory Networks.pdf:pdf},
title = {{Large-scale Simple Question Answering with Memory Networks}},
url = {http://arxiv.org/abs/1506.02075},
year = {2015}
}
@article{Chen2014,
abstract = {Almost all current dependency parsers classify based on millions of sparse indi-cator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed signif-icantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based depen-dency parser. Because this classifier learns and uses just a small number of dense fea-tures, it can work very fast, while achiev-ing an about 2{\%} improvement in unla-beled and labeled attachment scores on both English and Chinese datasets. Con-cretely, our parser is able to parse more than 1000 sentences per second at 92.2{\%} unlabeled attachment score on the English Penn Treebank.},
annote = {This is a very fast and accurate dependency parser that uses arc-shifts with a stack/buffer system. This is a transition-based dependency parser, as opposed to feature-based parsers. The transition-based parser uses part-of-speech (POS) tags, {\&}quot;compact dense vector representations of words{\&}quot;, and dependency labels. The key is that it learns via neural network; they use a cube activation function instead of the usual sigmoid or tanh functions, and this has higher accuracy. Additionally, the POS tags w/ label embeddings also perform better than previous state-of-the-art parsing systems.},
author = {Chen, Danqi and Manning, Christopher D},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Manning - 2014 - A Fast and Accurate Dependency Parser using Neural Networks.pdf:pdf},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
number = {i},
pages = {740--750},
title = {{A Fast and Accurate Dependency Parser using Neural Networks}},
url = {https://cs.stanford.edu/{~}danqi/papers/emnlp2014.pdf},
year = {2014}
}
@article{Gangwal2012,
author = {Gangwal, Gaurav},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Gangwal - 2012 - Question Answering System using Open Source Software.pdf:pdf},
title = {{Question Answering System using Open Source Software}},
url = {http://scholarworks.sjsu.edu/etd{\_}projects/258/},
year = {2012}
}
@article{Green2011,
author = {Green, N},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Green - 2011 - Dependency Parsing.pdf:pdf},
isbn = {9788073781842},
pages = {137--142},
title = {{Dependency Parsing}},
year = {2011}
}
@article{Grois2005,
author = {Grois, Eugene and Wilkins, David C.},
doi = {10.1145/1102351.1102384},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Grois, Wilkins - 2005 - Learning strategies for story comprehension.pdf:pdf},
isbn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning - ICML '05},
pages = {257--264},
title = {{Learning strategies for story comprehension}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102384},
year = {2005}
}
@article{Hermann2015,
abstract = {Teaching machines to read natural language documents remains an elusive chal-lenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03340v1},
author = {Hermann, Karm Moritz and Ko{\v{c}}isk{\'{y}}, Tom{\'{a}}{\v{s}} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
eprint = {arXiv:1506.03340v1},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Hermann et al. - 2015 - Teaching Machines to Read and Comprehend.pdf:pdf},
journal = {arXiv},
pages = {1--13},
title = {{Teaching Machines to Read and Comprehend}},
year = {2015}
}
@article{Hinton,
author = {Hinton, Geoffrey},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Hinton - Unknown - Deep Belief Nets.pdf:pdf},
title = {{Deep Belief Nets}}
}
@article{Kumar2015,
abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a unified neural network framework which processes input sequences and questions, forms semantic and episodic memories, and generates relevant an-swers. Questions trigger an iterative attention process which allows the model to condition its attention on the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state of the art results on sev-eral types of tasks and datasets: question answering (Facebook's bAbI dataset), sequence modeling for part of speech tagging (WSJ-PTB), and text classification for sentiment analysis (Stanford Sentiment Treebank). The model relies exclu-sively on trained word vector representations and requires no string matching or manually engineered features.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.07285v1},
author = {Kumar, Ankit and Irsoy, Ozan and Su, Jonathan and Bradbury, James and English, Robert and Pierce, Brian and Ondruska, Peter and Gulrajani, Ishaan and Socher, Richard},
eprint = {arXiv:1506.07285v1},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Kumar et al. - 2015 - Ask Me Anything Dynamic Memory Networks for Natural Language Processing.pdf:pdf},
journal = {arXiv},
pages = {1--10},
title = {{Ask Me Anything: Dynamic Memory Networks for Natural Language Processing}},
year = {2015}
}
@article{Hirschman1999,
author = {Hirschman, Lynette and Light, Marc and Breck, Eric and Burger, John D},
file = {:Users/SaahilM/Documents/Princeton/Academics/Thesis/ResearchPapers/DeepRead.pdf:pdf},
journal = {Proceedings of ACL},
pages = {325--332},
title = {Deep Read: A Reading Comprehension System},
year = {1999}
}
@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Quoc and Mikolov, Tomas},
eprint = {1405.4053},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Le, Mikolov - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
isbn = {9781634393973},
journal = {International Conference on Machine Learning - ICML 2014},
pages = {1188--1196},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{Manning2014,
author = {Manning, Christopher D and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven J and McClosky, David},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Manning et al. - 2014 - The Stanford CoreNLP Natural Language Processing Toolkit.pdf:pdf},
journal = {Proceedings of 52nd Annual Meeting of the ACL: System Demonstrations},
pages = {55--60},
title = {{The Stanford CoreNLP Natural Language Processing Toolkit}},
year = {2014}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
journal = {Nips},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{Mnih2007,
abstract = {The current study characterized the in vitro surface reactions of microroughened bioactive glasses and compared osteoblast cell responses between smooth and microrough surfaces. Three different bioactive glass compositions were used and surface microroughening was obtained using a novel chemical etching method. Porous bioactive glass specimens made of sintered microspheres were immersed in simulated body fluid (SBF) or Tris solutions for 1, 6, 24, 48, or 72 h, and the formation of reaction layers was studied by means of a scanning electron microscope/energy dispersive X-ray analysis (SEM/EDXA). Cell culture studies were performed on bioactive glass disks to examine the influence of surface microroughness on the attachment and proliferation of human osteoblast-like cells (MG-63). Cell attachment was evaluated by means of microscopic counting of in situ stained cells. Cell proliferation was analyzed with a nonradioactive cell proliferation assay combined with in situ staining and laser confocal microscopy. The microroughening of the bioactive glass surface increased the rate of the silica gel layer formation during the first hours of the immersion. The formation of calcium phosphate layer was equal between control and microroughened glass surfaces. In cell cultures on bioactive glass, the microrough surface enhanced the attachment of osteoblast-like cells but did not have an effect on the proliferation rate or morphology of the cells as compared with smooth glass surface. In conclusion, accelerated the early formation of surface reactions on three bioactive glasses and had a positive effect on initial cell attachment.},
author = {Mnih, a and Hinton, Ge},
doi = {10.1145/1273496.1273577},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Mnih, Hinton - 2007 - Three new graphical models for statistical language modelling.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th International Conference on Machine Learning (2007)},
pages = {641--648},
title = {{Three new graphical models for statistical language modelling.}},
url = {http://discovery.ucl.ac.uk/63252/},
volume = {62},
year = {2007}
}
@article{Narasimhan2015,
author = {Narasimhan, Karthik and Barzilay, Regina},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Narasimhan, Barzilay - 2015 - Machine Comprehension with Discourse Relations.pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
pages = {1253--1262},
title = {{Machine Comprehension with Discourse Relations}},
url = {http://www.aclweb.org/anthology/P15-1121},
year = {2015}
}
@article{Richardson2013,
abstract = {We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.},
author = {Richardson, Matthew and Burges, Christopher J C and Renshaw, Erin},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Richardson, Burges, Renshaw - 2013 - MCTest A Challenge Dataset for the Open-Domain Machine Comprehension of Text.pdf:pdf},
isbn = {9781937284978},
journal = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)},
number = {October},
pages = {193--203},
title = {{MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text}},
year = {2013}
}
@article{Sachan2015,
author = {Sachan, Mrinmaya and Dubey, Avinava and Xing, Eric P},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Sachan, Dubey, Xing - 2015 - Learning Answer-Entailing Structures for Machine Comprehension.pdf:pdf},
pages = {239--249},
title = {{Learning Answer-Entailing Structures for Machine Comprehension}},
year = {2015}
}
@article{Socher2011,
author = {Socher, Richard and Pennington, Jeffrey and Huang, Eh},
doi = {10.1.1.224.9432},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Socher, Pennington, Huang - 2011 - Semi-supervised recursive autoencoders for predicting sentiment distributions.pdf:pdf},
isbn = {978-1-937284-11-4},
issn = {1937284115},
journal = {Conference on Empirical Methods in Natural Language Processing, EMNLP},
number = {i},
pages = {151--161},
title = {{Semi-supervised recursive autoencoders for predicting sentiment distributions}},
url = {http://dl.acm.org/citation.cfm?id=2145450},
year = {2011}
}
@article{Sukhbaatar2015,
abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates slightly better performance than RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
archivePrefix = {arXiv},
arxivId = {1503.08895},
author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
eprint = {1503.08895},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Sukhbaatar et al. - 2015 - End-To-End Memory Networks.pdf:pdf},
pages = {1--11},
title = {{End-To-End Memory Networks}},
url = {http://arxiv.org/abs/1503.08895},
year = {2015}
}
@article{Turney2010,
abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
archivePrefix = {arXiv},
arxivId = {1003.1141},
author = {Turney, Peter D. and Pantel, Patrick},
doi = {10.1613/jair.2934},
eprint = {1003.1141},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Turney, Pantel - 2010 - From frequency to meaning Vector space models of semantics.pdf:pdf},
isbn = {1076-9757},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {141--188},
title = {{From frequency to meaning: Vector space models of semantics}},
volume = {37},
year = {2010}
}
@article{Weston2015,
abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
archivePrefix = {arXiv},
arxivId = {1502.05698},
author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Mikolov, Tomas and Rush, Alexander M.},
eprint = {1502.05698},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Weston et al. - 2015 - Towards AI-Complete Question Answering A Set of Prerequisite Toy Tasks.pdf:pdf},
title = {{Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks}},
url = {http://arxiv.org/abs/1502.05698},
year = {2015}
}
@article{Weston2015a,
abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
archivePrefix = {arXiv},
arxivId = {1410.3916v10},
author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
eprint = {1410.3916v10},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Weston, Chopra, Bordes - 2015 - Memory Networks.pdf:pdf},
journal = {International Conference on Learning Representations},
keywords = {Neural Network: convolutional,Neural Network: recurrent,Neural network: memory},
pages = {1--14},
title = {{Memory Networks}},
url = {http://arxiv.org/abs/1410.3916},
year = {2015}
}
@article{Yazdani2014,
author = {Yazdani, Majid and Henderson, James},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Yazdani, Henderson - 2014 - Incremental Recurrent Neural Network Dependency Parser with Search-based Discriminative Training.pdf:pdf},
pages = {142--152},
title = {{Incremental Recurrent Neural Network Dependency Parser with Search-based Discriminative Training}},
year = {2014}
}
