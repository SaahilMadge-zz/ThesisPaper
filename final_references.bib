@article{Bachman2014,
abstract = {We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout [9] in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We examine the relationship of pseudo-ensembles, which involve perturbation in model-space, to standard ensemble methods and existing notions of robustness, which focus on perturbation in observation-space. We present a novel regular- izer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer nat- urally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of [19] into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.4864v1},
author = {Bachman, Philip and Alsharif, Ouais and Precup, Doina},
eprint = {arXiv:1412.4864v1},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Bachman, Alsharif, Precup - 2014 - Learning with Pseudo-Ensembles.pdf:pdf},
journal = {Business},
pages = {1--9},
title = {{Learning with Pseudo-Ensembles}},
year = {2014}
}
@article{Bader2007,
abstract = {ASALSAN is a new algorithm for computing three-way DEDICOM, which is a linear algebra model for analyzing intrinsically asymmetric relationships, such as trade among nations or the exchange of emails among individuals, that incorporates a third mode of the data, such as time. ASALSAN is unique because it enables computing the three-way DEDICOM model on large, sparse data. A nonnegative version of ASALSAN is described as well. When we apply these techniques to adjacency arrays arising from directed graphs with edges labeled by time, we obtain a smaller graph on latent semantic dimensions and gain additional information about their changing relationships over time. We demonstrate these techniques on international trade data and the Enron email corpus to uncover latent components and their transient behavior. The mixture of roles assigned to individuals by ASALSAN showed strong correspondence with known job classifications and revealed the patterns of communication between these roles. Changes in the communication pattern over time, e.g., between top executives and the legal department, were also apparent in the solutions.},
author = {Bader, Bret and Harshman, Richard a. and Kolda, Tamara G.},
doi = {10.1109/ICDM.2007.54},
file = {:Users/SaahilM/Documents/Princeton/Academics/Thesis/ResearchPapers/Nickel2011.pdf:pdf},
isbn = {0-7695-3018-4},
issn = {1550-4786},
journal = {Seventh IEEE International Conference on Data Mining (ICDM 2007)},
keywords = {ASALSAN algorithm,Algorithm design and analysis,Data analysis,Data mining,International trade,Laboratories,Law,Legal factors,Linear algebra,US Department of Energy,USA Councils,adjacency arrays,directed graphs,large sparse data,linear algebra model,mathematics computing,matrix algebra,semantic graph temporal analysis,three-way DEDICOM computing},
pages = {33--42},
title = {{Temporal Analysis of Semantic Graphs Using ASALSAN}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4470227},
year = {2007}
}
@article{Berant2014,
author = {Berant, Jonathan and Clark, Peter},
file = {:Users/SaahilM/Documents/Princeton/Academics/Thesis/ResearchPapers/berant-srikumar-manning-emnlp14.pdf:pdf},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
pages = {1499--1510},
title = {{Modeling Biological Processes for Reading Comprehension}},
url = {http://allenai.org/content/publications/berant-srikumar-manning-emnlp14.pdf},
year = {2014}
}
@book{Bishop1995,
abstract = {This book provides a solid statistical foundation for neural networks from a pattern recognition perspective. The focus is on the types of neural nets that are most widely used in practical applications, such as the multi-layer perceptron and radial basis function networks. Rather than trying to cover many different types of neural networks, Bishop thoroughly covers topics such as density estimation, error functions, parameter optimization algorithms, data pre-processing, and Bayesian methods. All topics are organized well and all mathematical foundations are explained before being applied to neural networks. The text is suitable for a graduate or advanced undergraduate level course on neural networks or for practitioners interested in applying neural networks to real-world problems. The reader is assumed to have the level of math knowledge necessary for an undergraduate science degree. This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, C M},
doi = {10.2307/2965437},
eprint = {0-387-31073-8},
file = {:Users/SaahilM/Documents/Princeton/Academics/Thesis/ResearchPapers/Neural{\_}Networks{\_}for{\_}Pattern{\_}Recognition{\_}-{\_}Christopher{\_}Bishop.pdf:pdf},
isbn = {0198538642},
issn = {01621459},
journal = {Journal of the American Statistical Association},
pages = {482},
pmid = {1144972},
title = {{Neural networks for pattern recognition}},
volume = {92},
year = {1995}
}
@article{Bordes2014,
abstract = {This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields state-of-the-art results on a competitive benchmark of the literature.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.3676v1},
author = {Bordes, Antoine and Chopra, Sumit and Weston, Jason},
eprint = {arXiv:1406.3676v1},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Bordes, Chopra, Weston - 2014 - Question Answering with Subgraph Embeddings.pdf:pdf},
journal = {arXiv preprint arXiv:1406.3676},
number = {1},
pages = {615--620},
title = {{Question Answering with Subgraph Embeddings}},
url = {http://arxiv.org/abs/1406.3676},
year = {2014}
}
@article{Bordes2015,
abstract = {Training large-scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions. This paper studies the impact of multitask and transfer learning for simple question answering; a setting for which the reasoning required to answer is quite easy, as long as one can retrieve the correct evidence given a question, which can be difficult in large-scale conditions. To this end, we introduce a new dataset of 100k questions that we use in conjunction with existing benchmarks. We conduct our study within the framework of Memory Networks (Weston et al., 2015) because this perspective allows us to eventually scale up to more complex reasoning, and show that Memory Networks can be successfully trained to achieve excellent performance.},
archivePrefix = {arXiv},
arxivId = {1506.02075},
author = {Bordes, Antoine and Usunier, Nicolas and Chopra, Sumit and Weston, Jason},
eprint = {1506.02075},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Bordes et al. - 2015 - Large-scale Simple Question Answering with Memory Networks.pdf:pdf},
title = {{Large-scale Simple Question Answering with Memory Networks}},
url = {http://arxiv.org/abs/1506.02075},
year = {2015}
}
@article{Chang2014,
author = {Chang, Kai-Wei and Yih, Wen-tau and Yang, Bishan and Meek, Christopher},
file = {:Users/SaahilM/Documents/Princeton/Academics/Thesis/ResearchPapers/Trescal.pdf:pdf},
isbn = {9781937284961},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
pages = {1568--1579},
title = {{Typed tensor decomposition of knowledge bases for relation extraction}},
year = {2014}
}
@article{Chen2014,
abstract = {Almost all current dependency parsers classify based on millions of sparse indi-cator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed signif-icantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based depen-dency parser. Because this classifier learns and uses just a small number of dense fea-tures, it can work very fast, while achiev-ing an about 2{\%} improvement in unla-beled and labeled attachment scores on both English and Chinese datasets. Con-cretely, our parser is able to parse more than 1000 sentences per second at 92.2{\%} unlabeled attachment score on the English Penn Treebank.},
annote = {This is a very fast and accurate dependency parser that uses arc-shifts with a stack/buffer system. This is a transition-based dependency parser, as opposed to feature-based parsers. The transition-based parser uses part-of-speech (POS) tags, "compact dense vector representations of words", and dependency labels. The key is that it learns via neural network; they use a cube activation function instead of the usual sigmoid or tanh functions, and this has higher accuracy. Additionally, the POS tags w/ label embeddings also perform better than previous state-of-the-art parsing systems.},
author = {Chen, Danqi and Manning, Christopher D},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Manning - 2014 - A Fast and Accurate Dependency Parser using Neural Networks.pdf:pdf},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
number = {i},
pages = {740--750},
title = {{A Fast and Accurate Dependency Parser using Neural Networks}},
url = {https://cs.stanford.edu/{~}danqi/papers/emnlp2014.pdf},
year = {2014}
}
@article{Gangwal2012,
author = {Gangwal, Gaurav},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Gangwal - 2012 - Question Answering System using Open Source Software.pdf:pdf},
title = {{Question Answering System using Open Source Software}},
url = {http://scholarworks.sjsu.edu/etd{\_}projects/258/},
year = {2012}
}
@article{Green2011,
author = {Green, N},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Green - 2011 - Dependency Parsing.pdf:pdf},
isbn = {9788073781842},
pages = {137--142},
title = {{Dependency Parsing}},
year = {2011}
}
@article{Grois2005,
author = {Grois, Eugene and Wilkins, David C.},
doi = {10.1145/1102351.1102384},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Grois, Wilkins - 2005 - Learning strategies for story comprehension.pdf:pdf},
isbn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning - ICML '05},
pages = {257--264},
title = {{Learning strategies for story comprehension}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102384},
year = {2005}
}
@article{Hermann2015,
abstract = {Teaching machines to read natural language documents remains an elusive chal-lenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03340v1},
author = {Hermann, Karm Moritz and Ko{\v{c}}isk{\'{y}}, Tom{\'{a}}{\v{s}} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
eprint = {arXiv:1506.03340v1},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Hermann et al. - 2015 - Teaching Machines to Read and Comprehend.pdf:pdf},
journal = {arXiv},
pages = {1--13},
title = {{Teaching Machines to Read and Comprehend}},
year = {2015}
}
@article{Hinton,
author = {Hinton, Geoffrey},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Hinton - Unknown - Deep Belief Nets.pdf:pdf},
title = {{Deep Belief Nets}}
}
@article{Hirschman1999,
author = {Hirschman, Lynette and Light, Marc and Breck, Eric and Burger, John D},
file = {:Users/SaahilM/Documents/Princeton/Academics/Thesis/ResearchPapers/DeepRead.pdf:pdf},
journal = {Proceedings of ACL},
pages = {325--332},
title = {{{\{}D{\}}eep {\{}R{\}}ead: {\{}A{\}} Reading Comprehension System}},
year = {1999}
}
@article{Koren2009,
abstract = {As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels.},
archivePrefix = {arXiv},
arxivId = {ISSN 0018-9162},
author = {Koren, Y. and Bell, R. and Volinsky, C.},
doi = {10.1109/MC.2009.263},
eprint = {ISSN 0018-9162},
file = {:Users/SaahilM/Documents/Princeton/Academics/Thesis/ResearchPapers/ALS.pdf:pdf},
isbn = {0018-9162},
issn = {0018-9162},
journal = {Computer},
keywords = {Computational intelligence,Matrix factorization,Netflix Prize},
number = {8},
pages = {42--49},
pmid = {17255001},
title = {{Matrix Factorization Techniques for Recommender Systems}},
volume = {42},
year = {2009}
}
@article{Kumar2015,
abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a unified neural network framework which processes input sequences and questions, forms semantic and episodic memories, and generates relevant an-swers. Questions trigger an iterative attention process which allows the model to condition its attention on the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state of the art results on sev-eral types of tasks and datasets: question answering (Facebook's bAbI dataset), sequence modeling for part of speech tagging (WSJ-PTB), and text classification for sentiment analysis (Stanford Sentiment Treebank). The model relies exclu-sively on trained word vector representations and requires no string matching or manually engineered features.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.07285v1},
author = {Kumar, Ankit and Irsoy, Ozan and Su, Jonathan and Bradbury, James and English, Robert and Pierce, Brian and Ondruska, Peter and Gulrajani, Ishaan and Socher, Richard},
eprint = {arXiv:1506.07285v1},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Kumar et al. - 2015 - Ask Me Anything Dynamic Memory Networks for Natural Language Processing.pdf:pdf},
journal = {arXiv},
pages = {1--10},
title = {{Ask Me Anything: Dynamic Memory Networks for Natural Language Processing}},
year = {2015}
}
@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Quoc and Mikolov, Tomas},
eprint = {1405.4053},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Le, Mikolov - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
isbn = {9781634393973},
journal = {International Conference on Machine Learning - ICML 2014},
pages = {1188--1196},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{Manning2014,
author = {Manning, Christopher D and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven J and McClosky, David},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Manning et al. - 2014 - The Stanford CoreNLP Natural Language Processing Toolkit.pdf:pdf},
journal = {Proceedings of 52nd Annual Meeting of the ACL: System Demonstrations},
pages = {55--60},
title = {{The Stanford CoreNLP Natural Language Processing Toolkit}},
year = {2014}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
journal = {Nips},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{Mnih2007,
abstract = {The current study characterized the in vitro surface reactions of microroughened bioactive glasses and compared osteoblast cell responses between smooth and microrough surfaces. Three different bioactive glass compositions were used and surface microroughening was obtained using a novel chemical etching method. Porous bioactive glass specimens made of sintered microspheres were immersed in simulated body fluid (SBF) or Tris solutions for 1, 6, 24, 48, or 72 h, and the formation of reaction layers was studied by means of a scanning electron microscope/energy dispersive X-ray analysis (SEM/EDXA). Cell culture studies were performed on bioactive glass disks to examine the influence of surface microroughness on the attachment and proliferation of human osteoblast-like cells (MG-63). Cell attachment was evaluated by means of microscopic counting of in situ stained cells. Cell proliferation was analyzed with a nonradioactive cell proliferation assay combined with in situ staining and laser confocal microscopy. The microroughening of the bioactive glass surface increased the rate of the silica gel layer formation during the first hours of the immersion. The formation of calcium phosphate layer was equal between control and microroughened glass surfaces. In cell cultures on bioactive glass, the microrough surface enhanced the attachment of osteoblast-like cells but did not have an effect on the proliferation rate or morphology of the cells as compared with smooth glass surface. In conclusion, accelerated the early formation of surface reactions on three bioactive glasses and had a positive effect on initial cell attachment.},
author = {Mnih, a and Hinton, Ge},
doi = {10.1145/1273496.1273577},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Mnih, Hinton - 2007 - Three new graphical models for statistical language modelling.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th International Conference on Machine Learning (2007)},
pages = {641--648},
title = {{Three new graphical models for statistical language modelling.}},
url = {http://discovery.ucl.ac.uk/63252/},
volume = {62},
year = {2007}
}
@article{Narasimhan2015,
author = {Narasimhan, Karthik and Barzilay, Regina},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Narasimhan, Barzilay - 2015 - Machine Comprehension with Discourse Relations.pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
pages = {1253--1262},
title = {{Machine Comprehension with Discourse Relations}},
url = {http://www.aclweb.org/anthology/P15-1121},
year = {2015}
}
@article{Nickel2011,
abstract = {Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.},
author = {Nickel, Maximilian and Tresp, Volker and Kriegel, Hans-Peter},
file = {:Users/SaahilM/Documents/Princeton/Academics/Thesis/ResearchPapers/paper-icml2011.pdf:pdf},
isbn = {978-1-4503-0619-5},
journal = {28th International Conference on Machine Learning},
pages = {809----816},
title = {{A Three-Way Model for Collective Learning on Multi-Relational Data}},
year = {2011}
}
@Book{Nielsen2015,
author = {Nielsen, Michael A.},
title = {Neural Networks and Deep Learning},
publisher = {Determination Press},
year = {2015},
url = {http://neuralnetworksanddeeplearning.com/index.html}
}
@article{Nissen,
abstract = {www.software20.org 15 Software 2.0 2/2005 In the human brain the neurons are connected in a seem-ingly random order and send impulses asynchronously. If we wanted to model a brain this might be the way to organise an ANN, but since we primarily want to create a function approxi-mator, ANNs are usually not organised like this. When we create ANNs, the neurons are usually ordered in layers with connections going between the layers. The fi rst layer contains the input neurons and the last layer contains the output neurons. These input and output neurons represent the input and output variables of the function that we want to approximate. Between the input and the output layer a number of hidden lay-ers exist and the connections (and weights) to and from these hidden layers determine how well the ANN performs. When an ANN is learning to approximate a function, it is shown exam-ples of how the function works and the internal weights in the ANN are slowly adjusted so as to produce the same output as in the examples. The hope is that when the ANN is shown a new set of input variables, it will give a correct output. Therefore, if an ANN is expected to learn to spot a tumour in an X-ray image, it will be shown many X-ray images containing tumours, and many X-ray images containing healthy tissues. After a period of training with these images, the weights in the ANN should hopefully con-tain information which will allow it to positively identify tumours in X-ray images that it has not seen during the training.},
author = {Nissen, Steffen},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Fast Artifi cial Neural Network Library.pdf:pdf},
title = {{Fast Artifi cial Neural Network Library}}
}
@article{Richardson2013,
abstract = {We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.},
author = {Richardson, Matthew and Burges, Christopher J C and Renshaw, Erin},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Richardson, Burges, Renshaw - 2013 - MCTest A Challenge Dataset for the Open-Domain Machine Comprehension of Text.pdf:pdf},
isbn = {9781937284978},
journal = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)},
number = {October},
pages = {193--203},
title = {{MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text}},
year = {2013}
}
@article{Sachan2015,
author = {Sachan, Mrinmaya and Dubey, Avinava and Xing, Eric P},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Sachan, Dubey, Xing - 2015 - Learning Answer-Entailing Structures for Machine Comprehension.pdf:pdf},
pages = {239--249},
title = {{Learning Answer-Entailing Structures for Machine Comprehension}},
year = {2015}
}
@article{Socher2011,
author = {Socher, Richard and Pennington, Jeffrey and Huang, Eh},
doi = {10.1.1.224.9432},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Socher, Pennington, Huang - 2011 - Semi-supervised recursive autoencoders for predicting sentiment distributions.pdf:pdf},
isbn = {978-1-937284-11-4},
issn = {1937284115},
journal = {Conference on Empirical Methods in Natural Language Processing, EMNLP},
number = {i},
pages = {151--161},
title = {{Semi-supervised recursive autoencoders for predicting sentiment distributions}},
url = {http://dl.acm.org/citation.cfm?id=2145450},
year = {2011}
}
@article{Sukhbaatar2015,
abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates slightly better performance than RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
archivePrefix = {arXiv},
arxivId = {1503.08895},
author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
eprint = {1503.08895},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Sukhbaatar et al. - 2015 - End-To-End Memory Networks.pdf:pdf},
pages = {1--11},
title = {{End-To-End Memory Networks}},
url = {http://arxiv.org/abs/1503.08895},
year = {2015}
}
@article{Turney2010,
abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
archivePrefix = {arXiv},
arxivId = {1003.1141},
author = {Turney, Peter D. and Pantel, Patrick},
doi = {10.1613/jair.2934},
eprint = {1003.1141},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Turney, Pantel - 2010 - From frequency to meaning Vector space models of semantics.pdf:pdf},
isbn = {1076-9757},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {141--188},
title = {{From frequency to meaning: Vector space models of semantics}},
volume = {37},
year = {2010}
}
@article{Weston2015,
abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
archivePrefix = {arXiv},
arxivId = {1502.05698},
author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Mikolov, Tomas and Rush, Alexander M.},
eprint = {1502.05698},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Weston et al. - 2015 - Towards AI-Complete Question Answering A Set of Prerequisite Toy Tasks.pdf:pdf},
title = {{Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks}},
url = {http://arxiv.org/abs/1502.05698},
year = {2015}
}
@article{Weston2015a,
abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
archivePrefix = {arXiv},
arxivId = {1410.3916v10},
author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
eprint = {1410.3916v10},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Weston, Chopra, Bordes - 2015 - Memory Networks.pdf:pdf},
journal = {International Conference on Learning Representations},
keywords = {Neural Network: convolutional,Neural Network: recurrent,Neural network: memory},
pages = {1--14},
title = {{Memory Networks}},
url = {http://arxiv.org/abs/1410.3916},
year = {2015}
}
@article{Yazdani2014,
author = {Yazdani, Majid and Henderson, James},
file = {:Users/SaahilM/Library/Application Support/Mendeley Desktop/Downloaded/Yazdani, Henderson - 2014 - Incremental Recurrent Neural Network Dependency Parser with Search-based Discriminative Training.pdf:pdf},
pages = {142--152},
title = {{Incremental Recurrent Neural Network Dependency Parser with Search-based Discriminative Training}},
year = {2014}
}
